#!/usr/bin/python3

import argparse
import random
import glob
import os
import time
import json

import numpy as np
from keras import optimizers
from keras.callbacks import TensorBoard
from keras import backend as K
import tensorflow as tf
import cv2

import model
from utils import MapillaryGenerator, apply_color_map, make_parallel

parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=16, help='input batch size')
parser.add_argument('--image_width', type=int, default=640, help='the input image width')
parser.add_argument('--image_height', type=int, default=320, help='the input image height')
parser.add_argument('--n_gpu', type=int, default=1, help='Number of GPUs to use')
parser.add_argument('--n_cpu', type=int, default=8, help='Number of CPU threads to use during data generation')
parser.add_argument('--weights', type=str, default='weights.h5', help='path for the weights file')
parser.add_argument('--test_images', type=str, default='datasets/mapillary/testing/images/*.jpg', help='path to test images folder')
opt = parser.parse_args()

print(opt)

# Batch size has to be incremented by number of GPUs (each GPU processes batch_size samples)
opt.batch_size *= opt.n_gpu 

#### Test ####

# Workaround to avoid tensorflow from crashing the gpu
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)
K.set_session(sess)

# Model
net = model.build_bn(opt.image_width, opt.image_height, 66, weights_path=opt.weights, train=False)
net = make_parallel(net, opt.n_gpu)

# Testing
x = np.zeros((opt.batch_size, opt.image_height, opt.image_width, 3), dtype='float32')
image_paths = glob.glob(opt.test_images)
random.shuffle(image_paths)

for i, image_path in enumerate(image_paths[:opt.batch_size]):
    x[i,:,:,:] = cv2.resize(cv2.imread(image_path, 1), (opt.image_width, opt.image_height))

start_time = time.time()
y = net.predict_on_batch(x)
duration = time.time() - start_time

print('Generated %s segmentations in %s seconds -- %s FPS' % (opt.batch_size, duration, opt.batch_size/duration))

# Save output images to dir
with open('datasets/mapillary/config.json') as config_file:
    config = json.load(config_file)
labels = config['labels']

for i, segmentation in enumerate(y):
    image = apply_color_map(np.argmax(segmentation, axis=-1), labels)
    cv2.imwrite('output/%s.png' % (i), cv2.resize(image, (opt.image_width, opt.image_height)))
###############
